# Transit

* It is to be noted that the activations discovered are usually inverse of gradients. Therefore, it could be a good experiment to stabalize the whole graph using the product of both. 
* It is a strange discovery but the gradients of the Pythia for attention and mlp layer are same. 